{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19edf956",
   "metadata": {},
   "source": [
    "# E-commerce Algorithmic Analysis\n",
    "\n",
    "\n",
    "In my attempt to simulate the start of an e-commerce business, I generated 100 million products/sku's representing potential items to sell. From this data, I extract 2 million. For each item I generate a single sellability column that I envision to be correlated to profitability - competitiveness + popularity.\n",
    "\n",
    "Profitability represents the amount each product can be sold for, minus the cost to buy and import. Competitiveness represents a measure of the number of existing similar products with favorable reviews and fewer complaints. Popularity of a product measures the quantity of recent mentions across social media and other online venues.\n",
    "\n",
    "I generate categories/subcategories (generated by ChatGPT) for each item.\n",
    "\n",
    "  \n",
    "Tasks: \n",
    "\n",
    "1. Check for duplicates in data by uploading data into a pandas dataframe. Check using a naive nested loop on the column: O(n^2). More efficiently, make a \"set\" and return any entries that are duplicates: O(n). \n",
    "\n",
    "2. With a dictionary I can do an SKU lookup in O(1).\n",
    "\n",
    "3. Break the data by category, create a priority queue Minheap (O(n)) that maximizes sellability. Within each category, sort each heap for a total time complexity of O(nlogn).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be6856",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "- import libraries\n",
    "- import file with 2 million data entries \n",
    "- generate an array of SKU column for later use\n",
    "- generate a dictionary that maps each SKU to corresponding row for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a88813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from numba import njit\n",
    "import random\n",
    "import heapq\n",
    "\n",
    "df = pd.read_csv(\"generated_data.txt\").iloc[:2_000_000]\n",
    "df.columns = df.columns.str.replace(\" \", \"\", regex=False) ## credit chatGPT removing spaces from columns \n",
    "df_numpy = df[\"SKU\"].to_numpy()\n",
    "\n",
    "# generate dictionary (preparation for the O(1) algorithm) \n",
    "# Dictionary stores all the SKU's and maps to the corresponding row\n",
    "dict_1 = {}\n",
    "temp_columns = df.columns\n",
    "for i in range(len(df)):\n",
    "    temp_row = df.iloc[i]\n",
    "    if temp_row[\"SKU\"] not in dict_1:\n",
    "        dict_1[temp_row[\"SKU\"]] = {j : temp_row[j] for j in temp_columns[1:]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c6d19",
   "metadata": {},
   "source": [
    "# O(1)Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a916fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O(1) algorithm \n",
    "#Dictionary lookup for a given SKU\n",
    "def dict_lookup(SKU, dict = dict_1):\n",
    "    # generate random_SKU\n",
    "    if SKU in dict:\n",
    "        return dict[SKU]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60789cb0",
   "metadata": {},
   "source": [
    "# O(n) Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6384b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate search\n",
    "def duplicate_search_set(data):\n",
    "    #create a set\n",
    "    s = set()\n",
    "    #create a counter for number of duplicates\n",
    "    duplicates = 0\n",
    "    # create a list for list of duplicates\n",
    "    duplicate_list = []\n",
    "    #iterate through data and check against set\n",
    "    for i in data:\n",
    "        # Check if already in set\n",
    "        if i in s:\n",
    "            duplicates += 1\n",
    "            duplicate_list.append(i)\n",
    "        # Add element to set\n",
    "        s.add(i)\n",
    "    # return counter, duplicate list\n",
    "    return duplicates, duplicate_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5aa15",
   "metadata": {},
   "source": [
    "# O(nlogn) Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dcd51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heapify_categories(dataframe = df, column = \"Category\"):\n",
    "\n",
    "    # create a dictionary that maps a category to a dataframe\n",
    "    category_dict = {i:j for i,j in dataframe.groupby(\"Category\")}\n",
    " \n",
    "    #create dictionary mapping category to heap of rows in that category\n",
    "    heap_dict = {}\n",
    "    #create a second dictionary mapping category to sorted heap of rows in that category\n",
    "    heap_dict2 = {}\n",
    "\n",
    "    for category in category_dict: \n",
    "        # Create a list to store the heap\n",
    "        heap_list = []\n",
    "        # Create temp df that stores category dataframe \n",
    "        temp_df = category_dict[category]\n",
    "        # store each row from category-dataframe as a tuple with Sellability coming first (for heap)\n",
    "        for row in temp_df.itertuples():\n",
    "            #get index as a tie-breaker when comparing two items with same Sellability score\n",
    "            i = row.Index\n",
    "            #minimize sellabilty using inverse for minheap\n",
    "            heap_item = (1/row.Sellability, i, row)\n",
    "            #add tuple to heap_list\n",
    "            heap_list.append(heap_item)\n",
    "        #ADD heap_list to heap dictionary\n",
    "        heap_dict[category] = heap_list\n",
    "\n",
    "    #heapify each heap list\n",
    "    def heapify_dict(dict_of_lists):\n",
    "        for category in dict_of_lists:\n",
    "            heapq.heapify(dict_of_lists[category])\n",
    "        return\n",
    "    \n",
    "    heapify_dict(heap_dict)\n",
    "\n",
    "    #sort each heap\n",
    "    def sort_heaps(data):\n",
    "        #create list to store heap \n",
    "        sorted_heap = []\n",
    "        while data:\n",
    "            sorted_heap.append(heapq.heappop(data))\n",
    "        return sorted_heap\n",
    "\n",
    "    #use above function to sort each heap into heap_dict\n",
    "    for category in heap_dict:\n",
    "        # copies theheap_dict [category] so as not to sort in place\n",
    "        sorted_heap = sort_heaps(heap_dict[category].copy()) \n",
    "        heap_dict2[category] = sorted_heap\n",
    "\n",
    "    return heap_dict, heap_dict2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a36438",
   "metadata": {},
   "source": [
    "# O(n^2) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O(N^2 duplicate search algorithm)\n",
    "# The SKU_to_number algorithm comes from ChatGPT, to optimize the SKU data into unique numbers\n",
    "# For O(n^2) processing, 2 million rows were not feasible on my computer without optimization\n",
    "# The way it works is by creating a base 36 numbering system\n",
    "# 0-9 and A-Z map to a number 0-35. With each additional character, just as in base 10,\n",
    "# I multiply existing number by base, and add the value of the new character.\n",
    "# with each SKU as a unique number, I can use the JIT Numba module which speeds up the nested loop considerably\n",
    "# (this operation was previously impossible on a 2 million large data set on my computer prior.)\n",
    "\n",
    "def sku_to_number(sku):\n",
    "    charset = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    base = len(charset)\n",
    "    char_to_val = {c: i for i, c in enumerate(charset)}\n",
    "    \n",
    "    sku = sku.upper()\n",
    "    num = 0\n",
    "    for c in sku:\n",
    "        num = num * base + char_to_val[c]\n",
    "    return num\n",
    "\n",
    "sku_numeric = np.array([sku_to_number(i)for i in df_numpy])\n",
    "\n",
    "@njit\n",
    "def duplicate_search(data):\n",
    "    # create an empty list to store duplicates\n",
    "    duplicate_list = []\n",
    "    # create a counter of duplicates\n",
    "    duplicates = 0\n",
    "    length = len(data)\n",
    "    # nested loop, search for duplicates\n",
    "    for i in range(length):\n",
    "        for j in range(i+1,length):\n",
    "            if data[i] == data[j]:\n",
    "                #if you find one, add to counter, append to list\n",
    "                duplicates += 1\n",
    "                duplicate_list.append(data[i])\n",
    "    return duplicates, duplicate_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647c173",
   "metadata": {},
   "source": [
    "# Algorithms iterated and timed\n",
    "- ### for: n = 100_000 -> 2_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data size (to be plotted on x-axis)\n",
    "plotter_x = []\n",
    "# the time the n algorithm takes (to be plotted on y-axis)\n",
    "plotter_y_1 = []\n",
    "plotter_y_n = []\n",
    "plotter_y_n2 = []\n",
    "plotter_y_nlogn = []\n",
    "\n",
    "for size in range(100_000, 2_000_001, 100_000):\n",
    "    # create list of data sizes for x-axis:\n",
    "    plotter_x.append(size)\n",
    "\n",
    "    # O(1):\n",
    "    # hash lookup\n",
    "    # generate a random SKU\n",
    "    random_SKU = np.random.choice(df_numpy[:size])\n",
    "    print(f\"O(1) complexity, n = {size}:\")\n",
    "    start = time.perf_counter()\n",
    "    # print the result and the time, then append to time list for O(1)\n",
    "    print(f\"Dictionary lookup of {random_SKU}: {dict_lookup(random_SKU)}\")\n",
    "    print(f\"time elapsed: {(total_time := (time.perf_counter() - start))}\")\n",
    "    plotter_y_1.append(total_time)\n",
    "\n",
    "    # O(n):\n",
    "    # search for duplicates using a set:\n",
    "    print(f\"O(n) complexity, n = {size}:\")\n",
    "    start = time.perf_counter()\n",
    "    # print number of duplicates found, then append times to list for O(n)\n",
    "    print(f\"{duplicate_search_set(df_numpy[:size])[0]} duplicates found\")\n",
    "    print(f\"time elapsed: { ( total_time := (time.perf_counter() - start) ) }\")\n",
    "    plotter_y_n.append(total_time)\n",
    "\n",
    "    print(f\"O(nlogn) complexity, n = {size}\")\n",
    "    start = time.perf_counter()\n",
    "    # generate a sorted minheap list for each category\n",
    "    dict, dict_1 = heapify_categories(df[:size], column = \"Category\")\n",
    "    print(f\"time elapsed: {(total_time := (time.perf_counter() - start))}\")\n",
    "    plotter_y_nlogn.append(total_time)\n",
    "\n",
    "    # O(n**2):\n",
    "    # duplicate search using naive nested for loop\n",
    "    print(f\"O(n^2) complexity, n = {size}:\")\n",
    "    start = time.perf_counter()\n",
    "    print(f\"{duplicate_search(sku_numeric[:size])[0]} duplicates found\")\n",
    "    print(f\"time elapsed: {(total_time := (time.perf_counter() - start))}\")\n",
    "    plotter_y_n2.append(round(total_time, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69bd72",
   "metadata": {},
   "source": [
    "# Plot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(plotter_x), (plotter_y_n2), label = \"O(n^2)\")\n",
    "plt.plot(np.array(plotter_x), (plotter_y_n), label = \"O(n)\")\n",
    "plt.plot(np.array(plotter_x), (plotter_y_1), label= \"O(1)\")\n",
    "plt.plot(np.array(plotter_x), (plotter_y_nlogn), label = \"O(nlogn)\")\n",
    "plt.title(\"Time Complexity\")\n",
    "plt.xlabel(\"Data Size\")\n",
    "plt.ylabel(\"Time in seconds\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542630bd",
   "metadata": {},
   "source": [
    "# Discussion:\n",
    "\n",
    "The algorithms produced results as expected with a few caveats:\n",
    "- O(1), O(n), O(nlogn) took increasingly more time, however they didn't differ a whole lot at small data sizes. At 100,000 data entries, each completed their tasks in under 1 second. However, at 2 million entries, O(nlogn) took around 5 seconds, while O(n) took a third of a second, and O(1) clocked in at a thousandth of a second.\n",
    "- To gather the times, time.perf_counter was used at the start and finish for each algorithm.\n",
    "- O(n^2) took longer than the others by a wide margin. Even at 100K entries, it took over 3 seconds. As data grew, n^2 revealed a severe handicap. It was so slow that it became untenable, even for a dataset of only 2 Million. Two optimizations suggested by AI ended up making it feasible (but still slow): the Numba module which \"just in time\" compiles into machine code, and a string to number function which enabled Numba to work with data in the form of numbers rather than strings.\n",
    "- For big data sets where waiting days is not possible, choice of algorithm has an important impact. For 2 million data entries, and even with optimizations, the O(n^2) algorithm took north of 20 minutes while O(nlogn) took only 5 seconds. \n",
    "- It was suprising to see how useful and quick it is to hash O(1). With one pass across data, hashing provides a map to use in perpetuity for instant access. There was a hashing element involved in every data structure used for each of the algorithms above.\n",
    "- Mapping from category to sorted min heap took the most effort. The dataframe was separated into a dictionary of smaller dataframes, one per category. Using the smaller dataframes, a dictionary was constructed to key the category to a list of rows, and then from category to min-heap and sorted min-heap of rows. The purpose of the minheap for an e-commerce business, was that if interested in selling a particular product, it would be wise to consider similar but more sellable products in the same category space before making investments (which could result in more money without needing to pivot).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6571bc1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

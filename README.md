# E-commerce Algorithmic Analysis


In my attempt to simulate the start of an e-commerce business, I generated 100 million products/sku's representing potential items to sell. From this data, I extract 2 million. For each item I generate a single sellability column that I envision to be correlated to profitability - competitiveness + popularity.

Profitability represents the amount each product can be sold for, minus the cost to buy and import. Competitiveness represents a measure of the number of existing similar products with favorable reviews and fewer complaints. Popularity of a product measures the quantity of recent mentions across social media and other online venues.

I generate categories/subcategories (generated by ChatGPT) for each item.

  
Tasks: 

1. Check for duplicates in data by uploading data into a pandas dataframe. Check using a naive nested loop on the column: O(n^2). More efficiently, make a "set" and return any entries that are duplicates: O(n). 

2. With a dictionary I can do an SKU lookup in O(1).

3. Break the data by category, create a priority queue Minheap (O(n)) that maximizes sellability. Within each category, sort each heap for a total time complexity of O(nlogn).


# Discussion:

The algorithms produced results as expected with a few caveats:
- O(1), O(n), O(nlogn) took increasingly more time, however they didn't differ a whole lot at small data sizes. At 100,000 data entries, each completed their tasks in under 1 second. However, at 2 million entries, O(nlogn) took around 5 seconds, while O(n) took a third of a second, and O(1) clocked in at a thousandth of a second.
- To gather the times, time.perf_counter was used at the start and finish for each algorithm.
- O(n^2) took longer than the others by a wide margin. Even at 100K entries, it took over 3 seconds. As data grew, n^2 revealed a severe handicap. It was so slow that it became untenable, even for a dataset of only 2 Million. Two optimizations suggested by AI ended up making it feasible (but still slow): the Numba module which "just in time" compiles into machine code, and a string to number function which enabled Numba to work with data in the form of numbers rather than strings.
- For big data sets where waiting days is not possible, choice of algorithm has an important impact. For 2 million data entries, and even with optimizations, the O(n^2) algorithm took north of 20 minutes while O(nlogn) took only 5 seconds. 
- It was suprising to see how useful and quick it is to hash O(1). With one pass across data, hashing provides a map to use in perpetuity for instant access. There was a hashing element involved in every data structure used for each of the algorithms above.
- Mapping from category to sorted min heap took the most effort. The dataframe was separated into a dictionary of smaller dataframes, one per category. Using the smaller dataframes, a dictionary was constructed to key the category to a list of rows, and then from category to min-heap and sorted min-heap of rows. The purpose of the minheap for an e-commerce business, was that if interested in selling a particular product, it would be wise to consider similar but more sellable products in the same category space before making investments (which could result higher revenue without the need to pivot).
